h1. Goals

* Full support for *UTF-8* and all *ISO-8859-X* character sets / encodings
** ISO-8859-1 required for backward compatibility
** 8-bit character sets are more compact and allow faster regexp matching

* Relevant functionality:
** *regular expressions* (including POSIX character classes and regexp optimiser)
** *case/accent-folding* with @%c@ and @%d@ flags (regexp matching as well as @sort@, @count@ and @tabulate@ commands)
** appropriate *sorting* of query results (should use language- and country-specific locale)

* Proper input and output handling
** fixed character context in kwic output (@cat@ command), where the current implementation counts bytes instead of characters (and may thus break MBCs)
** strings in CQP queries accept hard-coded latex-style escapes for Latin1 accented characters, which should be deactivated (by default) and replaced by a more general mechanism for entering non-ASCII characters (@\xNN@, @\uNNNN@, etc.)
** interactive pager (@cat@, @count@, etc.) should automatically be configured for UTF-8 or ISO-8859-X character set
** in UTF-8 mode, all input strings (both interactive input in CQP and arguments of low-level CL functions) must be validated
** corpus indexing (@cwb-encode@) also has to make sure that UTF-8 strings are handled and sorted properly

* If possible, keep *compilation process* and *binary distribution* simple
** avoid dependencies on large & complex frameworks such as ICU or Glib
** static linking of external libraries, so stand-alone binaries can be distributed
** currently, it is possible to do Universal builds (@ppc@, @i386@ and @x86_64@ in a single binary) on Mac OS X, which is really nice

* Interactive users might want automatic recoding of input/output data
** set input and output character sets independently of encoding used by corpus
** otherwise, users would have to reconfigure their terminal window when they activate a new corpus with different encoding in a CQP session
** this feature is probably difficult to implement (it would probably use @libiconv@, but there seem to be lots of compatibility and robustness issues)
** probably easier to provide this functionality _only_ in HLL APIs (such as the @CWB::CQP@ module)

* Should the CQP command syntax allow identifiers with Unicode characters?
** CQP grammar: attribute names, named query results, etc.
** registry file grammar: attribute names, "long names" of corpora, comments, etc.
** *current recommendation* is to allow only simple ASCII identifiers (most portable, compatible with all supported character sets)
** note that corpus IDs, attribute names and named query results are also used as (parts of) filenames, which may break if they contain non-ASCII characters

h1. Glossary

* MBC = multi-byte character (in UTF-8 encoding)
* HLL = high-level language (Perl, Python, PHP, etc]
* "character set" and "encoding" are used interchangeably (although this is not formally correct); they always refer to one of the following: UTF-8, ISO-8859-X or ASCII

h1. Implementation steps

# Enfore declaration of encoding with @charset@ corpus property in registry files
** already implemented; default for missing declaration could be @latin1@ (backward compatibility) or @ascii@ (encourages upgrades)
# Upgrade *regular expression functions* in @cl/regopt.c@ to multi-charset implementation
** all regexp functionality in the CWB should already be encapsulated in @CL_Regex@ objects (check!)
** regexp optimiser (@cl_regopt_analyse()@, @cl_regex_match()@) should work out of the box for all ASCII extensions (in particular, ISO-8859-X and UTF-8)
** @cl_new_regex()@ has to make use of charset declaration, which is already part of the API
** alternative API: regexp tied to corpus handle, can only be used to match strings from this corpus
** @cl_regex_match()@ should validate input strings for correct encoding if possible (esp. with UTF-8)
# All low-level CL functions must validate MBCs in UTF-8 input strings to ensure well-defined behaviour
** virtually all functions are linked to a corpus handle, from which they obtain their expected character set
# Extend code in @cl/special_chars.c@ to support multiple character sets
** in particular, @%c@ and @%d@ mappings must be implemented for all supported character sets
** for ISO-8859-X, *mapping tables* have to be declared in analogy to @latin1_nocase_tab@ and @latin1_nodiac_tab@
** for UTF-8, *case-folding* is handled by external library (preferably with @towlower@, although Unicode defines a different folding algorithm, e.g. for German _Stra√üe_ vs. _STRASSE_)
** *accent-folding* in UTF-8 is tricky, and no support library seems to offer pre-defined functions for this operation; possible strategies: (1) decompose, strip combining characters, compose; (2) construct explicit mapping from Unicode name database
** case-folding and esp. accent-folding in UTF-8 will be rather slow; future CWB versions may want to store normalised versions of lexicon files on disk
# Revise *escape codes* for non-ASCII characters (@cl_string_latex2iso()@)
** existing latex escapes should be deactivated by default, but can be switched on for Latin1-encoded corpora with compatibility option
** note that latex escapes also make it impossible to quote an arbitrary input string (containing ' and ") safely
** new escape mechanism for non-ASCII characters: numeric codes (@\xNN@, @\uNNNN@) and possibly named characters in UTF-8 mode (@\N{LATIN SMALL LETTER A WITH GRAVE}@)
# Unicode string normalisation
** a crucial problem of Unicode is that many characters do not have a unique representation; e.g. latin letter + combining accent vs. pre-composed accented letter (see "Unicode normalization FAQ":http://unicode.org/faq/normalization.html)
** in order to ensure correct matching, all UTF-8 input strings have to be *normalised*
** applies in particular to CL low-level functions and the @cwb-encode@ / @cwb-s-encode@ tools
** recommended: canonical pre-composed form (*NFC*), which gives a more compact string representation (and seems to be used by most existing software)
** problem: many simple Unicode / regexp libraries don't offer normalisation functions
# CQP commands @sort@, @count@ and @tabulate@ should work out of the box, since they rely on CL string folding
** just need to make sure that the commands activate the appropriate character set
** ideally, sorting should use a local-specific collation algorithm (which differs by language and country)
# If CQP defines built-in functions for string processing (@cqp/builtins.c@), they must also be adapted to multiple character sets
# Proper handling of fixed-character context in *kwic output* (@cat@) will require a major rewrite
** affects all kwic-formatting code in @cqp/output.c@, @cqp/print-modes.c@, @ascii-print.c@, @html-print.c@, @latex-print.c@, @sgml-print.c@, etc.
** this code is inefficient and seriously broken anyway (buffer overflow + segfault for large context sizes), so it should be re-implemented from scratch
** recommendation: drop HTML, Latex and SGML modes; just offer ASCII for interactive use and XML as a general-purpose format (which can easily be transformed to other formats using XSLT, Perl, etc.)
# If a "heavy" Unicode support library (e.g. ICU) is used, local installation has to be included in binary distribution
** current binaries are statically linked with all non-standard libraries and can be installed and used stand-alone
** "heavy" packages (e.g. Glib) consist of multiple shared libraries, data files, and configuration information (and may depend on further libraries, e.g. Glib requires @libgettext@)
** for a standalone package, complete installations of these libraries have to be included in the binary distribution; wrapper scripts must set appropriate paths for dynamic libraries etc. before calling CWB tools; direct linking of e.g. CL library into other applications (as used by @CWB::CL@) may be very difficult

h1. Unicode software options (external libraries)

* Regular expressions
** easy solution: use standard *POSIX regexp functions* with locale support
*** requires minimal changes to existing CWB code
*** well-defined POSIX syntax with basic character classes, should be reasonably fast
** *ICU regular expressions* (performance? POSIX syntax only?)
*** performance? might be heavy-weight and slow
*** POSIX syntax only?
** *PCRE* library
*** very powerful regular-expression syntax
*** supports UTF-8 and single-byte codes (but no character classes in ISO-8859-X encodings?)
*** said to be relatively heavy-weight and much slower than Perl regexp
** *Oniguruma* regular expression library
*** modern regexp library, explicitly designed for multiple character sets
*** powerful syntax, but not Perl-compatible

* Support functions (string manipulation, case-folding)
** easy solution: standard *POSIX string functions* with locale support (should provide most of the necessary functionality)
** *ICU* library (certainly has everything we might need)
** *Glib* confirmed to include all necessary functions (but difficult to install & distribute)
** SQLite uses various Unicode support functions from *Tcl* source code
** is it possible to extract Unicode functionality from *Perl* codebase?

* Unicode normalisation and accent-folding
** *not available in POSIX* and most other light-weight solutions
** *ICU* should provide all kinds of normalisation functions
** other solutions: *Tcl* source code(?), or a specialised "utf8proc library":http://www.flexiguided.de/publications.utf8proc.en.html (not released under a standard license)

h1. Discussion of support libraries

* POSIX locales
** @+@ no dependency on external libraries
** @+@ easy to implement, with minimal changes to existing CWB code
** @+@ string length, lowercasing and regular expressions confirmed to work on several platforms 
** @?@ performance and reliability (across platforms) are unclear
** @-@ only plain POSIX regexp syntax without powerful Perl-style extensions
** @-@ need to activate appropriate locale (including language + country setting) for each string operation
** @-@ locale names are not standardised, which may lead to compatibility problems (idea: precise locale to be declared in registry file, otherwise CWB will try to guess a widespread locale name for chosen character set + language)
** @-@ may be difficult to port to Windows (but Cygwin works)

* ICU
** @+@ everything we need for Unicode support available in a single package
** @?@ performance is unclear
** @?@ do ICU regexp support only POSIX syntax or also Perl extensions?
** @-@ as bloated as a hippo after a bean-eating contest
** @-@ other functions / libraries are still needed for ISO-8859-X encodings
** @-@ as far as Stefan knows, ICU is a complex installation with multiple shared libraries and data files, so it cannot be statically linked in an easily distributable stand-alone binary package

* Glib
** @+@ seems to offer all necessary Unicode functionality, except for regular expressions
** @+@ also includes many other useful support functions that abstract away from platform dependencies
** @-@ rather difficult to install (depends on other shared libs like @libgettext@ and local installation paths in @pkg-config@)
** @-@ cannot be statically linked into binaries for distribution

* "PCRE":http://www.pcre.org/ 
** @+@ well-known and very powerful regular expresson dialect
** @+@ presumably well-tested, since it is used by many software packages
** @?@ as far as Stefan knows, PCRE has support at least for UTF-8 and byte-coded strings, but no further knowledge about ISO-8859-X character sets (so it wouldn't recognise accented characters there)
** @-@ PCRE is rumored to be rather slow (esp. compared to the fast Perl implementation)

* "Oniguruma":http://www.geocities.jp/kosako3/oniguruma/
** @+@ modern, elegant regular expression library (also used by various software packages)
** @+@ explicitly designed to support multiple character sets natively
** @+@ documentation written in Japanese ;-)
** @?@ performance is unclear, some comparative tests would be needed
** @-@ powerful regexp extensions, but syntax is not Perl-compatible

* Specialised solutions
** "utf8proc":http://www.flexiguided.de/publications.utf8proc.en.html is a small software library for Unicode normalisation (providing missing functionality in POSIX locale approach); MIT license, not backed by a large developer community
** Tcl Unicode functions (as used by SQLite) for various string operations; unclear whether normalisation functions are included
** is it possible to extract Perl's Unicode functionality?
